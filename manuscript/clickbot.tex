% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  english,
  ,jou,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Using dialogues to increase positive attitudes towards Covid-19 vaccines in a vaccine-hesitant UK population},
  pdfauthor={Charlotte O. Brand1 \& Tom Stafford1},
  pdflang={en-EN},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother
\shorttitle{Clickbot replication}
\usepackage{dblfloatfix}


\usepackage{csquotes}
\ifXeTeX
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\else
  \usepackage[main=english]{babel}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\fi
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Using dialogues to increase positive attitudes towards Covid-19 vaccines in a vaccine-hesitant UK population}
\author{Charlotte O. Brand\textsuperscript{1} \& Tom Stafford\textsuperscript{1}}
\date{}


\note{\textcolor{red}{This paper is a pre-print and has not yet been peer-reviewed}}

\authornote{

Contents of this author note autogenerated using Tenzing \url{https://martonbalazskovacs.shinyapps.io/tenzing/}

The authors made the following contributions. Charlotte O. Brand: Conceptualization, Project Administration, Data collection and analysis, Visualization, Writing - Original Draft Preparation, Writing - Review \& Editing; Tom Stafford: Conceptualization, Funding Acquisition, Writing - Review \& Editing.

Correspondence concerning this article should be addressed to Charlotte O. Brand, Department of Psychology, University of Sheffield, Sheffield, S1 1HD. E-mail: \href{mailto:c.brand@sheffield.ac.uk}{\nolinkurl{c.brand@sheffield.ac.uk}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} University of Sheffield, Department of Psychology}

\abstract{
Recently, Altay et al (2021) showed that five minutes of interaction with a chatbot led to increases in Covid-19 vaccination attitudes and intentions in a French population. Here we replicate this effect in a vaccine-hesitant, UK-based population. We attempt to isolate what made the chatbot condition effective by controlling the amount of information provided, the trustworthiness of the information, and the level of interactivity. Like Altay et al, our experiment allowed participants to navigate a branching dialogue by choosing questions of interest about Covid-19 vaccines. Our control condition used the same questions and answers but removed participant choice by presenting the dialogues at random. Importantly we also targeted those who were either against or neutral towards Covid-19 vaccinations to begin with, screening-out those with already positive attitudes. Replicating Altay et al, we found a similar size increase in positive attitudes towards vaccination, and in intention to get vaccinated. Unlike Altay et al, we found no difference between our two conditions: choosing the questions did not increase vaccine attitudes or intentions any more than our control condition. These results suggest that the attitudes of the vaccine hesitant are modifiable with exposure to in-depth, trustworthy and engaging dialogues.
}



\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Communicating the effectiveness, safety and necessity of vaccination to the general public is arguably one of science communication's most important and emblematic challenges. Appropriately, huge amounts of attention and research effort has been directed towards how to increase Covid-19 vaccination uptake. Due to the urgency and impact of the problem, a multi-pronged attack is warranted, and thus research rightly spans many different strategies, from pre-empting misinformation on social media (Vraga \& Bode, 2021), presenting information on the comparison of Covid-19 symptoms to vaccination side-effects (Thorpe et al., 2021), presenting information on the timeline of vaccine development (Thorpe et al., 2021), different styles of myth-busting (Challenger, Sumner, \& Bott, 2021), the use of social norms (Moehring et al., 2021), framing messaging in terms of individual risk preferences (Trueblood, Sussman, \& O'Leary, 2020) and even chatbots (Altay, Hacquin, Chevallier, \& Mercier, 2021), all with varying levels of success.

Although chatbots are usually used for aiding the completion of tasks, for example navigating website FAQs or purchasing personalised items (train tickets, flights), interest is growing in their ability to create engaging, human-like dialogue. One way in which chatbots could be used for attitude change is their ability to deliver counter-arguments to common questions or concerns. The use of chatbots to change attitudes has previously been explored in the context of GMO (Genetically modified organism) attitudes (Altay et al., 2022). This experiment found that the chatbot increased positive attitudes towards GMO foods compared to two comparisons; 1) a short description of GMOs 2) a description of the consensus scientific view, but, it did not have a positive effect compared to a third condition: a counterargument condition. In this counterargument condition, participants were exposed to all GMO beliefs and counterarguments at once, rather than choosing which counterarguments to interact with. This suggested that providing access to counterarguments, rather than choice of information, was the driving factor behind the success of the chatbot. They found that the positive attitudes were mediated by time spent in the conditions, and that people spent on average longer in the counterargument condition. Crucially, they also found that in the chatbot condition, for three out of four arguments, the best predictor for selecting a given argument was how negative their initial view towards it was, suggesting participants did seem to select arguments based on their concerns.

The idea that choice of information is important chimes with research into people's apparent preference for choosing their own actions, making their own decisions, and choosing what path to take, even foregoing monetary rewards to retain agency (Bobadilla-Suarez, Sunstein, \& Sharot, 2017). Domains as diverse as animal learning and robotic control have shown the importance of intrinsic motivations for agency, curiosity and control for understanding and enabling complex behaviour (Baldassarre et al., 2014). It is reasonable, therefore, to assume that a chatbot experience may be engaging and by turn convincing because it supports the participant in playing an active role in the dialogue, making choices about the aspects of the topic they explore.

As well as ensuring the information aligns with participant's interests, it is also crucial to communicate trust for successful public health communication (Cummings, 2014). Eiser, Stafford, Henneberry, and Catney (2009) studied public attitudes in response to communication about pollution where they lived, and found that those who didn't trust scientific communication tended to doubt that the scientists had their own interests at heart, rather than doubt their expertise. Furthermore, high trust in information from other sources, such as family and friends, was not based on a misperception of greater expertise, but on the (arguably accurate) perception that these groups had their interests at heart. Indeed, low trust in government is consistently one of the strongest predictors of vaccine hesitancy Larson et al. (2018). Evidently the effectiveness of communication interventions to increase vaccination intentions may be affected by how trustworthy the intervention is deemed to be.

This paper replicates recent success in increasing positive attitudes, and intentions to take, Covid-19 vaccines by using a chatbot (Altay et al., 2021). The chatbot study included participants from a random sample of French adults, whereas here we recruit vaccine-hesitant, UK-based adults only, and attempt to dissect what in particular it was about the chatbot that was effective. In particular, we wanted to test if choice of information is a crucial factor driving the effectiveness of the chatbot. . The French chatbot enabled participants to select frequently asked questions about Covid-19 vaccinations, and then presented participants with answers to those questions. The chatbot would then present follow-up questions and further counter-arguments. This was compared to a control condition in which the participants read 90 words of standard information from a government website. We wanted to investigate a variety of factors that may have been responsible for the increase in vaccination attitudes and intentions, such as a) the amount of information b) the time spent with the information c) the interactivity or choice of information and d) the trustworthiness of the information. The `chatbot' condition manifestly allowed participants greater choice, but it also exposed participants to a greater amount of information, and they tended to spend more time engaged as a consequence. The chatbot condition also included content on the trustworthiness of the information being presented, whereas the control condition didn't. As such, it isn't clear which underlying factors drive the observed effect.

To address our question of what drove the increase in positive vaccination attitudes and intentions, the current study uses the same information as Altay et al. (2021) but deploys two conditions in which the only difference is the interactivity of the information, i.e.~the ability to choose which information to view. This allows us to directly test whether the interactivity of the information was a driving factor behind the success of the chatbot, by comparing the results of our control and choice conditions. The amount of information (number of words), time spent on the information, and indicators of the trustworthiness of the information are the same in both our control and our choice conditions, allowing us to indirectly test whether these affect the success of the intervention, by comparing our results to Altay et al. (2021) 's results

\hypertarget{method}{%
\section{Method}\label{method}}

\hypertarget{preregistered-hypotheses}{%
\subsection{Preregistered hypotheses}\label{preregistered-hypotheses}}

Our hypotheses, predictions and analyses were preregistered before data collection at \url{https://osf.io/t4gav}. All of our data, code and analysis scripts are available at \url{https://github.com/lottybrand/clickbot_analysis}.

We hypothesised that the Choice Condition would show a greater increase in positive attitudes towards Covid-19 vaccines, due to the ability of participants to choose the information most interesting or important to them. A difference between conditions would be strong evidence that one of the important aspects of chatbots in changing attitudes is that they allow the participant to choose what information to engage with, aside from the trustworthiness and amount of information presented. This logic led to the following three pre-registered predictions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Increase in willingness to have a vaccine will be predicted by condition (those in the Choice Condition will be more likely to show an increase in their intention to take the vaccine).
\item
  There will be an interaction between condition and time of ratings, in that vaccine attitudes will be most positive in the Choice Condition in the Post-experiment ratings compared to the Pre-experiment ratings.
\item
  The Choice Condition will be rated as more engaging than the Control Condition.
\end{enumerate}

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

Based on Altay et al.~2021 we recruited 716 adult participants from the UK. Using the recruitment platform Prolific, we were able to prescreen for UK based participants aged between 18 and 65 who had previously answered that they were either ``Against'' the Covid-19 vaccinations, or ``Neutral'' towards the Covid-19 vaccinations (as opposed to ``For'' Covid-19 vaccinations). As there were 657 participants registered to Prolific who answered ``Against'' at the time of recruitment, we attempted to recruit as many from this pool as possible. We only recruited participants who answered ``Against'' for the first seven days of data collection, as per our pre-registration. This led to 479 participants who answered ``Against'' in total, and a remaining 237 who answered ``Neutral''. The mean age was 35, and 207 participants were male (502 female, 2 non-binary, 2 other, 3 prefer-not-to-say). 10 pilot participants were recruited on 26th April 2021 and their data used for pre-registering our analysis script only (they do not contribute data to the analyses presented here). The remaining participants were recruited between 14th May and 24th May 2021.

\hypertarget{materials}{%
\subsection{Materials}\label{materials}}

The baseline questionnaire was almost identical to Altay et al.~except that we opted to use a 7 point Likert scale as opposed to 5 points (Rhemtulla, Brosseau-Liard, \& Savalei, 2012). We asked participants to rate how strongly they agree with the following statements (from 1 = Strongly Disagree to 7= Strongly Agree): \emph{I think Covid-19 vaccines are safe}, \emph{I think Covid-19 vaccines are effective}, \emph{I think we've had enough time to develop Covid-19 vaccines}, \emph{I think we can trust those who produce Covid-19 vaccines}, \emph{I think it is important to be vaccinated against Covid-19}. We also asked participants if they had yet taken a dose of any Covid-19 vaccine (Yes, No) and whether they would consider taking any future dose of an approved Covid-19 vaccine offered to them (Yes, No, Undecided).

The information we used for our two conditions was taken from the Altay et al.~study. We translated the information into English using automated translation via Google Docs, proof-read it, updated it with the most recent information at the time using official UK NHS and Government sources (e.g.~regarding the Astra-Zeneca blood clot news), and had the information verified and fact-checked again by an independent epidemiologist.

To mimic the main features of their chatbot - interactive choice of questions and appropriate follow-up answers - we grouped the vaccine information into 5 main questions: 1) Is the vaccine safe? 2) Is the vaccine effective? 3) Has the vaccine been rushed? 4) Can we trust who makes the vaccine? 5) Is the vaccine necessary? Within each of the 5 main questions were four sub-questions. Thus there were 20 question-answer dialogues altogether, and each participant was presented with four out of those 20. We modified each sub-question to consist of a short dialogue of between 200-500 words largely avoiding repetition. Each dialogue included a short answer, and two or three follow-up question-answer pairs. (These documents, along with a document recording the main changes made to each section compared to the Altay paper can be found in the supplementary material and on the online repository). Thus our participants experienced almost identical information to Altay et al, in dialogue format. As with Altay et al's study, the participant experience lacked some features of full interactive chat: in both Altay et al and our study, participants were not able to freely type but chose questions from a given selection, and replies were not individually or uniquely composed. However, Altay et al's study did contain bot-like features, such as a symbol that the bot was ``typing'', and a chat-like window, which was not present in our study.

Crucially, participants in both our control and our chatbot condition were presented with the following information about the trustworthiness of the study at the start of the condition:

\emph{``Why should I trust you? - We are two independent researchers, Lotty Brand and Tom Stafford, funded by a research council, with no links to pharmaceutical companies or other competing interests. }\\
\emph{We are interested in learning about people's vaccine attitudes, in providing reliable information about vaccines, and learning about people's engagement with this information. }\\
\emph{All of the information in this study has been gathered via scientific articles and reports from the past 30 years of vaccine research, as well as the most recent studies on Covid-19. The information has been checked by experts in immunology and epidemiology as of May 12th 2021.''}

In contrast, Altay et al's chatbot featured trust as one of the main question options in their chatbot condition (``why should I trust you?''), with a response similar to our wording above. If trust drives effectiveness of vaccine interventions then this could have driven the difference between their conditions, rather than the presence/absence of a chatbot per se. We therefore removed this question and answer from the dialogue options and inserted it at the beginning of both conditions, to ensure all participants would see it regardless of condition or choice of information. This ensured the communication of trustworthiness of our information was consistent across both conditions.

Our post experiment questionnaire consisted of the same covid attitude questions as the pre-experiment questionnaire, as well as questions on how engaging the experience was and how clear the information was. We also asked how often participants discuss vaccination with those who disagree with them and how often they actively learn about vaccines (e.g.~via reading articles, listening to podcasts). Participants were finally asked if they would recommend our study to a friend (if yes, they were given the option to share a link via Twitter or Facebook- and we recorded the proportion that did), whether they would take part again in a month's time, their age, gender and education level.

We included an attention check question amongst both the pre-experiment questionnaire and our post experiment questionnaire (``We would like to check that you are paying careful attention to the information in this study. Please respond to the following item with `Somewhat agree'.''). We used both of these attention check answers alongside a free-response answer to check that participants were attending to the study information, i.e.~we only included those that passed both attention checks and provided coherent, relevant information in the free-response text boxes (free-response text boxes were used to collect data for a different study question).

\hypertarget{procedure}{%
\subsection{Procedure}\label{procedure}}

Participants were randomly assigned to either the Control or Choice Condition. Participants in both conditions provided informed consent (ethical approval provided by the University of Sheffield) before answering the pre-exposure questionnaire, interacting with the experimental material, and finally answering a post-exposure questionnaire.

In our Control Condition, participants viewed four randomly chosen dialogues of between 200-500 words each, one from each of the 5 possible domains of vaccination concern (\emph{1) Is the vaccine safe? 2) Is the vaccine effective? 3) Has the vaccine been rushed? 4) Can we trust who makes the vaccine? 5) Is the vaccine necessary?}).

In our Choice Condition, participants were able to choose four dialogues in total of between 200-500 words each, one from each of the 5 possible domains of vaccination concern, as above. Each of the 5 domains contained four sub-questions. Thus participants had four choices, with one choice from each of the 5 main domains each time. This ensured the amount of information that the participants were exposed to was the same as in the Control condition. The information is displayed identically between the two conditions, in 200-500 word chunks at a time, so the information should be equally engaging and easy to read. This was also to ensure a similar engagement time across both conditions. These controls attempt to isolate any effect of choice of information (`interactivity') as a cause of difference between the conditions.

\hypertarget{analysis}{%
\section{Analysis}\label{analysis}}

Our hypotheses, predictions and analyses were preregistered before data collection at \url{https://osf.io/t4gav}. All of our data, code and analysis scripts are available at \url{https://github.com/lottybrand/clickbot_analysis}.

All models were run using the Rethinking package in R for Bayesian models (McElreath, 2018). We include model parameters based on a priori preregistered hypotheses. Throughout the manuscript we report mean model coefficients with their 89\% credible intervals. Model parameters were said to have an effect on the model outcome if their 89\% credible interval did not cross zero. 89\% intervals are the default credible interval setting for the Rethinking package, as they discourage interpreting results in terms of binary null hypothesis significance testing (McElreath, 2018). 95\% intervals would not alter the interpretation of our results. When relevant, we used model comparison to aid interpretation of results. Models were said to be a better fit to the data if their WAIC (widely-applicable, or Wanatabe-Aike information criterion) value held the most weight out of all models tested.

Priors were chosen to be weakly regularising, in order to control for both under- and overfitting the model to the data (McElreath, 2018). All models were checked for convergence using convergence criteria such as Rhat values and effective sample sizes, as well as visual inspection of trace plots.

In line with our preregistration, we analysed whether participants increased their intention to be vaccinated using a Bayesian binomial regression model with an increase (either from `No' to `Undecided', or from `Undecided' to `Yes') coded as a 1 (did increase intention), and all other instances as zero (did not increase intention). We also analysed whether there was a reduction in the number of participants reporting that they would not get vaccinated, by modelling ``No' as 1, and all other responses as zero. The second approach was included after observing an increase in the percentage of participants changing from a ``No'' to another category that was similar to the increase that Altay et al found. Our analysis strategy differed slightly from Altay et al's, thus after we failed to find the condition effect they found, we performed an equivalent analysis to theirs Both of these approaches are reported below.

In line with our pre-registration, when modelling Likert scale vaccination attitude responses, as well as Likert scale engagement ratings, we used ordinal categorical multi-level models, with varying intercepts for who the rater was, and for Likert scale item. This allowed us to use each Likert scale item as the unit of analysis, rather than average over several items, in accordance with recommendations on how to treat Likert scale data Liddell \& Kruschke (2018). It also allows us to preserve and use all of the information and variation, and account for data clustering within items and individuals (McElreath, 2018).

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{preregistered-hypotheses-1}{%
\subsection{Preregistered hypotheses}\label{preregistered-hypotheses-1}}

We found that participants reporting that they did not intend to get the vaccine decreased after our experiment, regardless of condition, as the number of those reporting they would not get the vaccine decreased in the post-exposure measure (mean model estimate: -0.36, 89\% Credible Interval: -0.53, -0.19 ). Against prediction 1, those in the Choice Condition were not more likely to increase their intention to have the vaccine compared to the Control Condition (mean: -0.22, 89\% CI: -0.57, 0.14). These shifts in intention can be seen in Table \ref{tab:intchange} and are equivalent to those found in Altay et al's chatbot condition; in Altay et al's chatbot condition 36\% of participants reported that they did not intend to get vaccinated, and this dropped to 29\% afterwards. Across both our conditions, 53\% reported that they did not intend to get vaccinated and this dropped to 44\% afterwards.

\begin{table}

\caption{\label{tab:intchange}Number of participants reporting that they don't (No), are undecided, or do (Yes) intend to get vaccinated pre and post exposure to the dialogues in each condition}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & No & Undecided & Yes\\
\hline
Choice (Pre) & 194 & 93 & 70\\
\hline
Choice (Post) & 167 & 107 & 83\\
\hline
Control (Pre) & 189 & 95 & 75\\
\hline
Control (Post) & 150 & 124 & 85\\
\hline
\end{tabular}
\end{table}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../plots/raw_density} 

}

\caption{Density plot of raw vaccination attitudes before and after the experiment. 7 = Strongly Agree and 1 = Strongly Disagree with the 5 vaccination items: i) vaccines are safe, ii) vaccines are effective, iii) they have not been rushed, iv) those who make them can be trusted, and v) they are necessary}\label{fig:rawdensity}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../plots/before_and_after_violin} 

}

\caption{Violin plot of average vaccination attitudes before and after the experiment. The green lines show an increase in positive vaccination attitudes, and red lines show a decrease}\label{fig:beforeafter}
\end{figure}

We also found that vaccine attitudes increased across both conditions (mean: 2.00 , 89\% CI: 1.93 , 2.07 ). Against prediction 2, as there was not an interaction between condition and time of ratings in our full model (mean: -0.09 , 89\% CI: -0.24, 0.04 ); vaccine attitudes were not most positive in the Post-Treatment ratings of the Choice Condition, but increased similarly in both conditions. This interpretation was confirmed by a model comparison approach, in which we compared models including parameters for condition, post-treatment rating, and an interaction between condition and post-treatment rating. The best fitting model included only the experiment effect, with the worst fitting models containing the interaction effect, and just varying intercepts (null model), suggesting that the experiment effect (change across both conditions) was most informative in predicting the difference in vaccination attitudes (see supplementary material)). Increase in average vaccination attitudes can be seen in the violin plot in Figure \ref{fig:beforeafter}.

This change in vaccination attitudes is displayed in Figure \ref{fig:rawdensity}, which shows the raw vaccination attitude ratings before and after the experiment. Figures displaying the differences in vaccination attitudes within different scale items (e.g.~are they safe, are they effective, have they been rushed, can we trust those who makes them, are they necessary) can be found in the supplementary material. These figures suggest that the majority of our sample agreed that vaccines are effective, but were undecided as to whether they are safe, and disagreed that we can trust those who produce them, that there has been enough time to produce them, and that they are necessary.

Against prediction 3, we did not find that the Choice Condition was rated as more engaging than the Control Condition (mean: 0.16 , 89\% CI: -0.03, 0.35 ).

\hypertarget{exploratory-analysis}{%
\subsection{Exploratory analysis}\label{exploratory-analysis}}

Overall, we found that the number of people who said that ``No'' they would not get a vaccination when one was offered to them decreased after taking part in our experiment. Out of 571 participants reporting that they either wouldn't, or were undecided, about getting the vaccine, 93 reported being more likely to get vaccinated after the experiment (16\% increase). Out of these 93, 6 changed directly from a ``No'' to a ``Yes,'' 25 went from an ``Undecided'' to a ``Yes,'' and 62 went from a ``No'' to an ``Undecided.''

As (Altay et al., 2021) found a stronger effect for those who spent the most time with the chatbot, we wanted to check whether a condition effect was present in those who spent more time with the information. The median amount of time spent viewing the information was 4 minutes, and we found that participants who spent above the median amount of time viewing the information (between 4 and 16 minutes, so between 1 - 4 minutes per dialogue), were more likely to increase their vaccination attitudes compared to those who spent less time viewing the information. We found a positive interaction between those who spent above the average amount of time and their post-treatment rating (mean: 0.48 , 89\% CI: 0.34, 0.61 ). This was confirmed by model comparison, in which the model including the interaction effect, as well as a main effect for post-treatment rating, was the best fitting model (details in supplementary material).

When looking only at those who spent above the median amount of time with the information, we again found no effect of condition on intention to get vaccinated (mean: -0.17 , 89\% CI: -0.64, 0.30 ).

In contrast, participants who spent above the average amount of time viewing the information were not more likely to show an increase in their intention to get vaccinated compared to the rest of the participants (mean: 0.21 , 89\% CI: -0.15, 0.57 ).

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

We ran an experiment to test if choice of information is a crucial factor driving the effectiveness of a Covid-19 vaccination chatbot. We recruited 716 adults based in the UK who had previously said they were ``against'' or ``neutral'' towards Covid-19 vaccines. Based on a chatbot experiment conducted with French participants (Altay et al., 2021), we created 20 dialogues split across the five topics; how safe the vaccines are, how effective they are; whether there has been enough time to develop them, whether we can trust who makes them; and whether they are necessary for young and healthy people. Participants were randomly assigned to two conditions; in one they could choose the dialogues they saw (Choice Condition), in the other the dialogues were randomly displayed (Control Condition). Overall, we found that, in both conditions, participants' vaccination attitudes and intentions shifted in a more positive direction after reading the dialogues; we found no difference between the Choice and Control Condition. Crucially we found that participants who spent above the average (median) amount of time viewing the information (between 4 and 16 minutes, or between 1 - 4 minutes per dialogue), were more likely to increase their vaccination attitudes than those who spent below the average (median) amount of time viewing the information. This association between viewing time and increased change was not found for intentions.

Our results have implications in light of recent interest in using chatbots or other interventions to increase vaccination uptake. We conclude that creating an engaging experience for participants that encourages them to spend quality time with the information is key for increasing positive attitudes towards vaccination.

The size of the shift in intentions we observed was similar to the results of the Altay et al.~chatbot condition. In this sense we provide a conceptual replication of their results. This is reassuring as we used identical information to theirs, only editing the information to be more appropriate for a UK-based audience and with the latest epidemiological information at the time. In both their and our experiment, we found an effect of time spent with the information, in that those who spent longer with the chatbot were more likely to increase their vaccination attitudes. This has potentially important implications for those designing public health information interventions, in that how engaging the material is (and therefore how long participants are willing to attend to the information) is crucial.

In contrast to the Altay experiment, we found no difference in effectiveness between our conditions. However, there were crucial differences between our conditions and those of Altay's. The most obvious is that all of our participants saw information of the same length and quality. The fact that our conditions were equally effective then suggests that Altay's chatbot may have been more effective than their control condition not because there is something inherently effective about chatbots per se, but simply because it delivered more information than the control condition, as supported by Altay et al. (2022). The second crucial difference between our experiment and Altay's is that we controlled for trustworthiness of information across both of our conditions. In Altay's chatbot experiment, the chatbot included a question ``why should I trust you?'' in which, if participants chose it, they saw information about who the researchers were and what their motives were. Previous research suggests trust plays a huge role in how effective science communication is Lindholt et al. (2021). The information in Altay's control condition was therefore implicitly less trustworthy than the chatbot information, given the control condition had no source and was anonymous. In contrast, both of our conditions included the ``Why should I trust you?'' information at the start of the experiment, before any of the other dialogues were displayed. This included who we (the authors) are, where the information came from, and what our motives are. We also stated that we had no links to pharmaceutical companies or any other vested interests. The fact that both of our conditions included this information on trustworthiness, and that both of our conditions were similarly effective at increasing positive attitudes and intentions, , implicitly suggests that being transparent about the source of information could be a crucial component for shifting vaccine attitudes and intentions. Of course, because our conditions did not differ in this way, this needs to be experimentally verified in future work. Nevertheless, the indirect comparison to Altay's results, in which the chatbot contained trust information and was more effective than the control that didn't, further suggests communicating trust could be an important factor.

By design, the only difference between the current study's two conditions was that in the experimental condition participants had a choice over which information they saw, whereas in the control condition the information was shown at random. This suggests that having agency or ``choice'' over the information one engages with may not be the most crucial aspect of why chatbots are effective. It suggests that addressing the concerns that are of most importance or interest to the participant may not be as crucial as previously thought, although it is important to note that all information was originally chosen to address common concerns of the vaccine-hesitant. Previous research suggests participants prefer choice and agency over information when given the choice, but perhaps this preference isn't enough to override the effectiveness of accurate and relevant information in general. Importantly, we did not find a difference in engagement ratings between our conditions, and participants spent a similar amount of time across both conditions. Again, when we compare to Altay et al's chatbot, we see that participants spent longer with their chatbot on average than with our information, and that time spent on the task is related to change in attitudes. These comparisons suggest that chatbots are most effective because of their ability to hold the attention of the participant, and thus spend more time engaging with the information. . Seemingly unimportant details of chatbots may account for their being engaging, t than standard text, for example, the `social' element of interacting with another `agent' may be inherently more engaging, or simply the display of the information, which is often more `bitesize' and delivered one sentence at a time.

It could be argued that our results are simply demonstrating a regression to the mean, particularly because we recruited from one end of the vaccination attitude spectrum, and we saw similar effects across both conditions. However, after investigating this possibility, it seems unlikely given that those who were rated as ``against'' vaccination as opposed to ``neutral'' were actually more likely to stay the same in their reported intention to get the vaccine than the neutral participants, and were less likely than the neutral participants to increase their intention to get vaccinated (i.e the opposite of what you would expect with regression to the mean). A plot displaying this is included in the supplementary material. Furthermore, not only are our percentage changes very similar to Altay's chatbot condition effects, who were recruited from the general population and not specifically against vaccination, but also much greater than those in previous studies, for example when influenced by norms, participants only showed a 5\% decline in rating themselves as `undecided' or `against' vaccination (Moehring et al., 2021), whereas we found a 16\% decline. Previous research also suggests that using Q\&A style information is more effective than presenting pure fact-based information, again reporting similar effects to ours (Challenger et al., 2021).

One aspect of our study worth noting is how the information was framed and how the participants were addressed throughout the study. Participants were asked if they were either Against, For, or Neutral towards the Covid-19 vaccines, as it is worded in Prolific's pre-screening criteria. We thus used this as our wording, and advertised the study as ``Your opinions on Covid-19 vaccinations.'' Part of our study (results not included for this publication) was to ask participants to imagine and put forward the opposite side's reasons for and against vaccination (this was conducted after their second round of attitude and intention measures, so would not influence the results of this study). We also offered participants an opportunity to provide any other feedback they had in an `anything else' box. These comments were insightful, and often hinted that participants were keen to have an outlet for their views. Anonymity perhaps allowed them to be honest, and we also noted many thanked us for not referring to them, or anyone, as ``anti-vaxxers.'' We refrained from using this term throughout as this term is often used to stereotype or villainise those who hold those views. We wish to follow-up these comments, respond where necessary, and share them with the rest of the research community to help further destigmatise those who are vaccine hesitant, and help create an atmosphere of constructive dialogue and conversational receptiveness about these issues (Yeomans, Minson, Collins, Chen, \& Gino, 2020). Comments are included in the Shiny App available at \url{https://lottybrand.shinyapps.io/vaccineComments/} .

Overall, we suggest it is important when designing science communication interventions to control for the amount of information, time spent with the information, trustworthiness of information, and consequently to ensure a high level of engagement with the information. Simply providing Q\&A-style dialogues appeared to be as effective as delivering the same information via a chatbot, and more effective than previous studies using norms or simple fact-based interventions.

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

Thank you to Dr Andrew Lee from the University of Sheffield who kindly donated his epidemiological expertise and proof-read our updated Covid-19 vaccination FAQs.
Thank you also to Dr Sacha Altay for openly sharing the data and analysis scripts, and being forthcoming with more details and extra information to help run this replication. The authors are funded by the EPSRC project EP/T024666/1 ``Opening Up Minds: Engaging Dialogue Generated From Argument Maps''

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-altay2021information}{}}%
Altay, S., Hacquin, A.-S., Chevallier, C., \& Mercier, H. (2021). Information delivered by a chatbot has a positive impact on COVID-19 vaccines attitudes and intentions. \emph{Journal of Experimental Psychology: Applied}.

\leavevmode\vadjust pre{\hypertarget{ref-altay2022scaling}{}}%
Altay, S., Schwartz, M., Hacquin, A.-S., Allard, A., Blancke, S., \& Mercier, H. (2022). Scaling up interactive argumentation by providing counterarguments with a chatbot. \emph{Nature Human Behaviour}, \emph{6}(4), 579--592.

\leavevmode\vadjust pre{\hypertarget{ref-badman2021}{}}%
Badman, R., Wang, A. X., Skrodzki, M., Cho, H., Aguilar-Lleyda, D., Shiono, N., \ldots{} Akaishi, R. (2021). Trust in institutions, not in political leaders, determines covid-19 public health compliance in societies across the globe. \url{https://doi.org/10.31234/osf.io/4dy8a}

\leavevmode\vadjust pre{\hypertarget{ref-baldassarre2014intrinsic}{}}%
Baldassarre, G., Stafford, T., Mirolli, M., Redgrave, P., Ryan, R. M., \& Barto, A. (2014). Intrinsic motivations and open-ended development in animals, humans, and robots: An overview. \emph{Frontiers in Psychology}, \emph{5}, 985.

\leavevmode\vadjust pre{\hypertarget{ref-bobadilla-suarez_intrinsic_2017}{}}%
Bobadilla-Suarez, S., Sunstein, C. R., \& Sharot, T. (2017). The intrinsic value of choice: {The} propensity to under-delegate in the face of potential gains and losses. \emph{Journal of Risk and Uncertainty}, \emph{54}(3), 187--202. \url{https://doi.org/10.1007/s11166-017-9259-x}

\leavevmode\vadjust pre{\hypertarget{ref-burkner2019ordinal}{}}%
Bürkner, P.-C., \& Vuorre, M. (2019). Ordinal regression models in psychology: A tutorial. \emph{Advances in Methods and Practices in Psychological Science}, \emph{2}(1), 77--101.

\leavevmode\vadjust pre{\hypertarget{ref-challenger_covid-19_2021}{}}%
Challenger, A. R., Sumner, P., \& Bott, L. (2021). \emph{{COVID}-19 myth-busting: An experimental study} (preprint). In Review. \url{https://doi.org/10.21203/rs.3.rs-790299/v1}

\leavevmode\vadjust pre{\hypertarget{ref-chevallier2021covid}{}}%
Chevallier, C., Hacquin, A.-S., \& Mercier, H. (2021). COVID-19 vaccine hesitancy: Shortening the last mile. \emph{Trends in Cognitive Sciences}.

\leavevmode\vadjust pre{\hypertarget{ref-cummings2014trust}{}}%
Cummings, L. (2014). The {``trust''} heuristic: Arguments from authority in public health. \emph{Health Communication}, \emph{29}(10), 1043--1056.

\leavevmode\vadjust pre{\hypertarget{ref-eiser_trust_2009}{}}%
Eiser, J. R., Stafford, T., Henneberry, J., \& Catney, P. (2009). {``{Trust} me, {I}'m a {Scientist} ({Not} a {Developer})''}: {Perceived} {Expertise} and {Motives} as {Predictors} of {Trust} in {Assessment} of {Risk} from {Contaminated} {Land}. \emph{Risk Analysis}, \emph{29}(2), 288--297. \url{https://doi.org/10.1111/j.1539-6924.2008.01131.x}

\leavevmode\vadjust pre{\hypertarget{ref-jennings2021lack}{}}%
Jennings, W., Stoker, G., Bunting, H., Valgarsson, V. O., Gaskell, J., Devine, D., \ldots{} Mills, M. C. (2021). Lack of trust, conspiracy beliefs, and social media use predict COVID-19 vaccine hesitancy. \emph{Vaccines}, \emph{9}(6), 593.

\leavevmode\vadjust pre{\hypertarget{ref-kamal2021rapid}{}}%
Kamal, A., Hodson, A., \& Pearce, J. M. (2021). A rapid systematic review of factors influencing COVID-19 vaccination uptake in minority ethnic groups in the UK. \emph{Vaccines}, \emph{9}(10), 1121.

\leavevmode\vadjust pre{\hypertarget{ref-larson2018measuring}{}}%
Larson, H. J., Clarke, R. M., Jarrett, C., Eckersberger, E., Levine, Z., Schulz, W. S., \& Paterson, P. (2018). Measuring trust in vaccination: A systematic review. \emph{Human Vaccines \& Immunotherapeutics}, \emph{14}(7), 1599--1609.

\leavevmode\vadjust pre{\hypertarget{ref-liddell2018analyzing}{}}%
Liddell, T. M., \& Kruschke, J. K. (2018). Analyzing ordinal data with metric models: What could possibly go wrong? \emph{Journal of Experimental Social Psychology}, \emph{79}, 328--348.

\leavevmode\vadjust pre{\hypertarget{ref-lindholt2021public}{}}%
Lindholt, M. F., Jørgensen, F., Bor, A., \& Petersen, M. B. (2021). Public acceptance of COVID-19 vaccines: Cross-national evidence on levels and individual-level predictors using observational data. \emph{BMJ Open}, \emph{11}(6), e048172.

\leavevmode\vadjust pre{\hypertarget{ref-maraqa2021covid}{}}%
Maraqa, B., Nazzal, Z., Rabi, R., Sarhan, N., Al-Shakhra, K., \& Al-Kaila, M. (2021). COVID-19 vaccine hesitancy among health care workers in palestine: A call for action. \emph{Preventive Medicine}, \emph{149}, 106618.

\leavevmode\vadjust pre{\hypertarget{ref-mcelreath2018statistical}{}}%
McElreath, R. (2018). \emph{Statistical rethinking: A bayesian course with examples in r and stan}. Chapman; Hall/CRC.

\leavevmode\vadjust pre{\hypertarget{ref-moehring_surfacing_2021}{}}%
Moehring, A., Collis, A., Garimella, K., Rahimian, M. A., Aral, S., \& Eckles, D. (2021). Surfacing norms to increase vaccine acceptance, 39.

\leavevmode\vadjust pre{\hypertarget{ref-rhemtulla2012can}{}}%
Rhemtulla, M., Brosseau-Liard, P. É., \& Savalei, V. (2012). When can categorical variables be treated as continuous? A comparison of robust continuous and categorical SEM estimation methods under suboptimal conditions. \emph{Psychological Methods}, \emph{17}(3), 354.

\leavevmode\vadjust pre{\hypertarget{ref-roozenbeek2020susceptibility}{}}%
Roozenbeek, J., Schneider, C. R., Dryhurst, S., Kerr, J., Freeman, A. L., Recchia, G., \ldots{} Van Der Linden, S. (2020). Susceptibility to misinformation about COVID-19 around the world. \emph{Royal Society Open Science}, \emph{7}(10), 201199.

\leavevmode\vadjust pre{\hypertarget{ref-thorpe_communicating_2021}{}}%
Thorpe, A., Fagerlin, A., Butler, J., Stevens, V., Drews, F. A., Shoemaker, H., \ldots{} Scherer, L. D. (2021). \emph{Communicating about {COVID}-19 vaccine development and safety} (p. 2021.06.25.21259519). \url{https://doi.org/10.1101/2021.06.25.21259519}

\leavevmode\vadjust pre{\hypertarget{ref-trueblood_role_2020}{}}%
Trueblood, J. S., Sussman, A. B., \& O'Leary, D. (2020). \emph{The {Role} of {General} {Risk} {Preferences} in {Messaging} {About} {COVID}-19 {Vaccine} {Take}-{Up}} (\{SSRN\} \{Scholarly\} \{Paper\} No. ID 3649654). Rochester, NY: Social Science Research Network. \url{https://doi.org/10.2139/ssrn.3649654}

\leavevmode\vadjust pre{\hypertarget{ref-uslu2021}{}}%
Uslu, A., Lazer, D., Perlis, R. H., Baum, M., Quintana, A., Ognyanova, K., \ldots{} al., et. (2021). \url{https://doi.org/10.31219/osf.io/fazup}

\leavevmode\vadjust pre{\hypertarget{ref-vraga_addressing_2021}{}}%
Vraga, E. K., \& Bode, L. (2021). Addressing {COVID}-19 {Misinformation} on {Social} {Media} {Preemptively} and {Responsively}. \emph{Emerging Infectious Diseases}, \emph{27}(2), 396--403. \url{https://doi.org/10.3201/eid2702.203139}

\leavevmode\vadjust pre{\hypertarget{ref-yeomans2020conversational}{}}%
Yeomans, M., Minson, J., Collins, H., Chen, F., \& Gino, F. (2020). Conversational receptiveness: Improving engagement with opposing views. \emph{Organizational Behavior and Human Decision Processes}, \emph{160}, 131--148.

\end{CSLReferences}


\end{document}
