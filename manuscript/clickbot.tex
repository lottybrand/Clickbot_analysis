% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  english,
  ,jou,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Time spent with fact-checked Covid-19 vaccine dialogues lead to a 15\% increase in intentions to get vaccinated in a UK-based anti-vaccination population},
  pdfauthor={Charlotte O. Brand1 \& Tom Stafford1},
  pdflang={en-EN},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother
\shorttitle{Clickbot replication}
\usepackage{dblfloatfix}


\usepackage{csquotes}
\ifxetex
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\else
  \usepackage[main=english]{babel}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\fi
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{Time spent with fact-checked Covid-19 vaccine dialogues lead to a 15\% increase in intentions to get vaccinated in a UK-based anti-vaccination population}
\author{Charlotte O. Brand\textsuperscript{1} \& Tom Stafford\textsuperscript{1}}
\date{}


\note{\textcolor{red}{This paper is a pre-print and has not yet been peer-reviewed}}

\authornote{

Contents of this author note autogenerated using Tenzing \url{https://martonbalazskovacs.shinyapps.io/tenzing/}

The authors made the following contributions. Charlotte O. Brand: Conceptualization, Project Administration, Data collection and analysis, Visualization, Writing - Original Draft Preparation, Writing - Review \& Editing; Tom Stafford: Conceptualization, Funding Acquisition, Writing - Review \& Editing.

Correspondence concerning this article should be addressed to Charlotte O. Brand, Enter postal address here. E-mail: \href{mailto:c.brand@sheffield.ac.uk}{\nolinkurl{c.brand@sheffield.ac.uk}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} University of Sheffield, Department of Psychology}

\abstract{
Recently, Altay et al (2020) showed that five minutes of interaction with a chatbot led to increases in Covid-19 vaccination attitudes and intentions, compared to a brief control condition. Here we partially replicate and qualify this effect, showing the importance of the choice of control condition. We introduce strict controls between the chatbot and non-chatbot conditions of the amount of information provided, the time spent with the information, how the trustworthiness of the information is communicated, and the level of interactivity. Like Altay et al, our chatbot condition allowed participants to navigate a branching dialogue by choice of set questions, eliciting set answers on aspects of the covid-19 vaccine. Our control condition used the same questions and answers but removed all elements of participant choice. In this way, our experiment isolated the effect of participant choice of information. To maximise the stringency of our study, we specifically targeted those who were already either against or neutral towards covid-19 vaccinations, screening-out those with positive attitudes. Matching Altay et al, we found an increase in positive attitudes towards vaccination, as well as an increase in intention to get vaccinated, after engaging with vaccine information. Unlike Altay et al, we found no difference between our conditions: choosing the questions did not increase vaccine attitudes, nor did it increase vaccine intentions, compared to display of unchoosen information. In common with Altay et al, we found time spent with the information, across both conditions, in that those who spent between 4 and 16 minutes (above the median) with the information were more likely to increase both their attitudes and their vaccination intentions. These results suggest that the attitudes of the vaccine hesitant are modifiable with exposure to in-depth vaccine information, presented in dialogue form, but that neither control of the interaction nor the act of choosing the questions to ask, are crucial factors.
}



\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Communicating the effectiveness, necessity and safety of vaccination to the general public is arguably one of science communication's most important and emblematic challenges. Appropriately, huge amounts of attention and research effort has been directed towards how to increase Covid-19 vaccination uptake. Due to the urgency and impact of the problem, a multi-pronged attack is warranted, and thus research rightly spans many different strategies, from pre-empting misinformation on social media (Vraga and Bode (2021)), presenting information on the comparison of Covid-19 symptoms to vaccination side-effects (Thorpe et al. (2021)), presenting information on the timeline of vaccine development (Thorpe et al. (2021)), different styles of myth-busting (Challenger, Sumner, and Bott (2021)), the use of social norms (Moehring et al. (n.d.)), framing messaging in terms of individual risk preferences (Trueblood, Sussman, and O'Leary (2020)) and even chatbots (Altay, Hacquin, Chevallier, and Mercier (2021)), all with varying levels of success.

Although chatbots are usually used for aiding the completion of tasks, for example navigating website FAQs or purchasing personalised items (train tickets, flights), interest is growing in their ability to create engaging, human-like dialogue. One way in which chatbots could be used for attitude change is their ability to deliver counter-arguments to common questions or concerns. One reason is the ability to address counter-arguments. The use of chatbots to change attitudes has previously been explored in the context of GMO attitudes (Altay et al. (2020)). This experiment found that the chatbot had a positive effect on GMO attitudes compared to two comparisons; 1) a short description of GMOs 2) a description of the consensus scientific view, but, it did not have a positive effect compared to a third condition: a counterargument condition. In this counterargument condition, participants were exposed to all GMO beliefs and counterarguments at once, rather than choosing which counterarguments to interact with. This suggested that providing access to counterarguments, rather than choice of information, was the driving factor behind the success of the chatbot. They found that the positive attitudes were mediated by time spent in the conditions, and that people spent on average longer in the counterargument condition. Crucially, they also found that in the chatbot condition, for three out of four arguments, the best predictor for selecting a given argument was how negative their initial view towards it was, suggesting participants did seem to select arguments based on their concerns.

This idea that choice of information is important chimes with research into people's apparent preference for choosing their own actions, making their own decisions, and choosing what path to take, even foregoing monetary rewards to retain agency (Bobadilla-Suarez, Sunstein, and Sharot (2017)). The importance of intrinsic motivation, agency, curiousity, control (Baldassarre et al. (2014)
). TK Should I talk more about choice here, or merge this with the above? Could we add the ``ikea effect'' idea here? I couldn't find any refs to back it up though.

This paper focuses on a recent study that found that information delivered by a chatbot increased positive attitudes towards Covid-19 vaccination compared to a control condition in which participants read 90 words (Altay, Hacquin, Chevallier, and Mercier (2021)). The chatbot enabled participants to select frequently asked questions about Covid-19 vaccinations, and then presented participants with information to those questions. The chatbot would then present follow-up questions and further counter-arguments. We wanted to investigate precisely what it was about the chatbot that increased vaccination attitudes, specifically whether it is the a) amount of information b) time spent with the information c) interactivity or choice of information or d) trustworthiness of the information. The most obvious differences between the chatbot condition and the control condition was the amount of information presented, and consequently the time spent with that information. However, two other important differences could have also been driving the effect: the role of trust and the role of choice.

Role of trust ``why should I trust you'' section here (Eiser, Stafford, Henneberry, and Catney (2009)). TK Should I put the choice part down here instead? Tom: No

To address our question of what precisely about the chatbot drove the increase in positive vaccination attitudes, we used the same information but created two conditions in which the only difference was the interactivity of the information. The amount of information (number of words), time spent on the information, and trustworthiness of the information was the same for both conditions.

\hypertarget{method}{%
\section{Method}\label{method}}

\hypertarget{preregistered-hypotheses}{%
\subsection{Preregistered hypotheses}\label{preregistered-hypotheses}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Increase in willingness to have a vaccine is predicted by condition (those in the choice condition are more likely to show an increase in their likeliness of taking the vaccine).
\item
  There is an interaction between condition and time of ratings, in that vaccine attitudes will be most positive in the Choice Condition in the Post-Treatment ratings compared to the Pre-Treatment ratings.
\item
  The Choice Condition will be rated as more engaging than the Control Condition.
\end{enumerate}

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

Based on Altay et al.~2021 we recruited 716 adults participants from the UK. Using the recruitment platform Prolific, we were able to prescreen for UK based participants aged between 18 and 65 who had previously answered that they were either ``Against'' the Covid-19 vaccinations, or ``Neutral'' towards the Covid-19 vaccinations (as opposed to ``For'' Covid-19 vaccinations). As there were 657 participants registered to Prolific who answered ``Against'' at the time of recruitment, we attempted to recruit as many from this pool as possible. We only recruited participants who answered ``Against'' for the first seven days of data collection, as per our pre-registration. This led to 479 participants who answered ``Against'' and 237 who answered ``Neutral.'' The mean age was 35 and 207 participants were male (502 female, 2 non-binary, 2 other, 3 prefer-not-to-say). 10 pilot participants were recruited on 26th April and their data used for pre-registering our analysis script. The remaining participants were recruited between 14th May and 24th May 2021.

\hypertarget{materials}{%
\subsection{Materials}\label{materials}}

The baseline questionnaire was almost identical to Altay et al.~except that we opted to use a 7 point Likert scale as opposed to 5 points (TK). We asked participants to rate how strongly they agree with the following statements (from 1 = Strongly Disagree to 7= Strongly Agree), I think Covid-19 vaccines are safe, I think Covid-19 vaccines are effective, I think we've had enough time to develop Covid-19 vaccines, I think we can trust those who produce Covid-19 vaccines, I think it is important to be vaccinated against Covid-19. We also asked participants if they had yet taken a dose of any Covid-19 vaccine (Y/N) and whether they would consider taking any future dose of an approved Covid-19 vaccine offered to them (Yes, No, Undecided).

The information we used for our two conditions was taken from the Altay et al.~study. We translated the information into English using Google Docs, proof-read it, updated it with the most recent information at the time using official UK NHS and Government sources, and had the information verified again by an independent epidemiologist.

Technical limitations prevented us using an identical platform to that used by Altay et al.~To mimic the main features of their chatbot - interactive choice of questions and eliciting appropriate answers - we grouped their vaccine information into 5 main questions: 1) Is the vaccine safe? 2) Is the vaccine effective? 3) Has the vaccine been rushed? 4) Can we trust who makes the vaccine? 5) Is the vaccine necessary? We modified each section to consist of a short dialogue of between 200-500 words largely avoiding repetition. Each dialogue included a short answer, and two or three follow-up question-answer responses. These documents, along with a document recording the main changes made to each section compared to the Altay paper can be found here xxx). Thus our participants experienced the same information as Altay et al, in dialogue format. TK say something about how our platform is not like a messaging app that you might imagine? But then Altays et al wasn't either, in important respects

At the beginning of each study, participants in both conditions saw a Question-Answer response saying:

\emph{``Why should I trust you? - We are two independent researchers, Lotty Brand and Tom Stafford, funded by a research council, with no links to pharmaceutical companies or other competing interests. }\\
\emph{We are interested in learning about people's vaccine attitudes, in providing reliable information about vaccines, and learning about people's engagement with this information. }\\
\emph{All of the information in this study has been gathered via scientific articles and reports from the past 30 years of vaccine research, as well as the most recent studies on Covid-19. The information has been checked by experts in immunology and epidemiology as of May 12th 2021.''}

This is because this was one of the main questions present in the French chatbot (``why should I trust you?'') and felt it could have been leading the difference in the chatbot's effectiveness (ref). We therefore removed this question and answer from the dialogue options and inserted it at the beginning of both conditions, so all participants would see it in both conditions. This ensured the level of trust in us and our information was consistent across both conditions.

Our post exposure questionnaire consisted of the same covid attitude questions as the pre-exposure questionnaire, as well as questions on how engaging the experience was and how clear the information was. We also asked how often they discuss vaccination with those who disagree with them and how often they actively learn about vaccines (e.g.~via reading articles, listening to podcasts). Would recommend to a friend? Take part again in a month's time? Age, gender, education level.

We also had an attention check question amongst both the pre-exposure questionnaire and our post-exposure questionnaire (``We would like to check that you are paying careful attention to the information in this study. Please respond to the following item with `Somewhat agree.'\,''). We used both of these attention check answers alongside a free-response answer to check that participants were attending to the study information carefully.

\hypertarget{procedure}{%
\subsection{Procedure}\label{procedure}}

Participants were randomly assigned to either the Control or Choice Condition. Participants in both conditions provided informed consent (ethical approval provided by the University of Sheffield approval code xxxx) before answering a baseline questionnaire, interacting with the experimental material, and finally answering a questionnaire after the exposure.

In our Control Condition, participants viewed four randomly chosen dialogues of between 200-500 words each, out of 5 possible domains of vaccination concern (are they safe, are they effective, were they rushed, can we trust who makes them, do I need to get one?).

In our Choice Condition, participants were able to choose four dialogues in total of between 200-500 words each, out of 5 possible domains of vaccination concern, as above. Therefore the amount of information (and therefore the amount of time) that the participants were exposed to should be equivalent between the Control and the Choice condition, leaving only the effect of Choice of information (and `interactivity') to be examined.

The information is displayed identically, in 200-500 word chunks at a time, thus the information itself should be equally engaging and easy to read.

We predict that the choice condition will show a greater effect due to the ability of participants to address the concerns most important to them. If we do find a difference, this is strong evidence that one of the important aspects of chatbots in changing attitudes is that it allows the participant to choose what information is addressed, aside from trust and amount of information.

\hypertarget{analysis}{%
\section{Analysis}\label{analysis}}

Our hypotheses, predictions and analyses were preregistered before data collection at \url{https://osf.io/t4gav}. All of our data, code and analysis scripts are available at www.github.com/lottybrand/clickbot\_analysis

All models were run using the Rethinking package in R (McElreath (2018)).

We include model parameters based on a-priori preregistered hypotheses. Model parameters were said to have an effect on the model outcome if their 89\% credible interval did not cross zero. 89\% intervals are the default credible interval setting for the Rethinking package, as they discourage interpreting results in terms of binary null-hypothesis-significance-testing (McElreath (2018)). 95\% intervals would not alter the interpretation of our results. When relevant, we used model comparison to aid interpretation of results. Models were said to be a better fit to the data if their WAIC (widely-applicable, or Wanatabe-Aike information criterion) value held the most weight out of all models tested.
Priors were chosen to be weakly regularizing, in order to control for both under- and overfitting the model to the data (McElreath (2018)). All models were checked for convergence using convergence criteria such as Rhat values and effective sample sizes, as well as visual inspection of trace plots.

In line with our preregistration, we analysed whether participants increased their intention to be vaccinated using a Bayesian binomial regression model with an increase (either from `No' to `Undecided,' or from `Undecided' to `Yes') coded as a 1 (did increase intention), and all other instances as zero (did not increase intention). We also analysed whether there was a reduction in the number of participants reporting that they would not get vaccinated, by modelling `No's' as 1, and all other responses as zero. The second approach was included as it allowed us to more directly compare our results with Altay's, after we failed to find a condition effect. Both of these approaches are reported below.

In line with our preregistration, when modelling Likert scale vaccination attitude responses, as well as Likert scale engagement ratings, we used ordinal categorical multi-level models, with varying intercepts for who the rater was, and for Likert scale item. This allowed us to use each Likert scale item as the unit of analysis, rather than average over several items, in accordance with recent recommendations on how to treat Likert scale data. (Bürkner and Vuorre (2019), (\textbf{lidell2019analyzing?}))

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{preregistered-hypotheses-1}{%
\subsection{Preregistered hypotheses}\label{preregistered-hypotheses-1}}

We found that participants' intention to have the vaccine increased after our experiment, as the number of those reporting they would not get the vaccine decreased overall (mean model estimate: -0.37, 89\% Credible Interval: -0.54, -0.20 ). Against prediction 1, those in the choice condition were not more likely to show an increase in their intention to have the vaccine compared to the control condition (mean: -0.23, 89\% CI: -0.57, 0.11). This can be seen in Table 1.

TK tables! \url{https://rmarkdown.rstudio.com/lesson-7.html} and \url{https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html}

(How to make this table? couldn't find example in your example. For it to be comparable to Altay's you'd want to stack \texttt{r\ table(clean\_clickbot\$choice\_cond,\ clean\_clickbot\$vax\_future\_1)} and \texttt{table(clean\_clickbot\$choice\_cond,\ clean\_clickbot\$vax\_future\_2)} on top of each other)

This change in vaccination attitudes is displayed in Figure 1 \ref{fig:figure1}, which shows the raw vaccination attitude ratings before and after the experiment. Figures displaying the differences in vaccination attitudes between different scale items can be found in the supplementary material.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../plots/raw_density} 

}

\caption{Density plot of raw vaccination attitudes before and after the experiment}\label{fig:figure1}
\end{figure}

Increase in average vaccination attitude can be seen in the violin plot in Figure 2 \ref{fig:figure2}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../plots/before_and_after_violin} 

}

\caption{Violin plot of average vaccination attitudes before and after the experiment}\label{fig:figure2}
\end{figure}

Vaccine attitudes increased similarly across both conditions (mean: 2.00 , 89\% CI: 1.93 , 2.07 ). This was against prediction 2, as there was not an interaction between condition and time of ratings; vaccine attitudes were not most positive in the Post-Treatment ratings of the Choice Condition, but increased similarly in both conditions (mean: 0.50 , 89\% CI: 0.40, 0.59 ). This interpretation was confirmed by a model comparison approach, in which we compared models including parameters for condition, post-treatment rating, and an interaction between condition and post-treatment rating. The best fitting model included only a post-treatment effect (need to reload all those other models too to pull out the WAICS here)

Against prediction 3, we did not find that the Choice Condition was rated as more engaging than the Control Condition (mean: 0.15 , 89\% CI: -0.04, 0.35 ).

\hypertarget{exploratory-analysis}{%
\subsection{Exploratory analysis}\label{exploratory-analysis}}

Although the majority of participants did not change their mind, overall we saw a 13\% reduction in the number of people reporting that they did not intend to get vaccinated. In those who spent between 4 and 16 minutes reading the information, this increased to 15\%

As (Altay, Hacquin, Chevallier, and Mercier (2021)) found a stronger effect for those who spent the most time with the chatbot, we wanted to check whether a condition effect was present in those who spent more time with the information. The median amount of time spent viewing the information was 4 minutes, however we found no difference in the effect of the conditions in those who spent above the median amount of time (between 4 and 16 minutes, so between 1 - 4 minutes per dialogue), with the information (mean: -0.17 , 89\% CI: -0.64, 0.30 ).

Participants who spent above the average amount of time viewing the information were not more likely to show an increase in their intention to get vaccinated (mean: 0.21 , 89\% CI: -0.15, 0.57 ).

However, participants who spent above the average amount of time viewing the information were more likely to increase their vaccination attitudes and intentions as we found a positive interaction between those who spent above the average amount of time and the post experiment rating (mean: 0.48 , 89\% CI: 0.34, 0.61 ). This was confirmed by model comparison, in which the model including an interaction effect for post-treatment rating and above median time, as well as a main effect for post-treatment rating, was the best fitting model (WAIC stuff)

TK What about a direct comparison of the size of effects between Altays chatbot condition and our conditions?

TK also mention the fact that even the vaccine hesitant tended to have mixed profiles of the vaccine subattitudes?

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

We ran an experiment to test if choice of information affected the effectiveness of Question and Answer (Q\&A) dialogues on popular Covid-19 vaccination concerns. We recruited over 700 adults based in the UK who had previously said they were ``against'' or ``neutral'' towards Covid-19 vaccines. Based on a chatbot experiment conducted with French participants, we created 20 Q\&A dialogues split across the five topics; how safe the vaccines are, how effective they are; whether there has been enough time to develop them, whether we can trust who makes them; and whether they are necessary for young and healthy people. Participants were randomly assigned to two conditions; in one they could choose the Q\&A dialogues they saw (Choice Condition), in the other the Q\&A dialogues were randomly displayed (Control Condition). Overall, we found that participants' vaccination attitudes shifted in a more positive direction after reading the dialogues in both conditions; we found no difference between the Choice and Control Condition. Crucially we found that participants who spent above the average (median) amount of time viewing the information (between 4 and 16 minutes, or between 1 - 4 minutes per dialogue), were more likely to increase their vaccination attitudes and intentions. We discuss the implications of our results in light of recent interest in using chatbots or other interventions to increase vaccination uptake. We argue that creating an engaging experience for participants that encourages them to spend quality time with the information is key for increasing positive attitudes towards vaccination.

Overall, we found that the number of people who said that ``No'' they would not get a vaccination when one was offered to them decreased after taking part in our experiment. Out of 716 people, 93 were more likely to get vaccinated after the experiment. Out of these 93, 6 went directly from a ``No'' to a ``Yes,'' 25 went from an ``Undecided'' to a ``Yes,'' and 62 went from a ``No'' to an ``Undecided.'' Although the majority of participants did not change their mind, overall we saw a 13\% reduction in the number of people reporting that they did not intend to get vaccinated. In those who spent between 4 and 16 minutes reading the information, this increased to 15\%. These figures are similar to the results of the Altay et al.~chatbot condition that this study was based on, providing a conceptual replication of their results. This is reassuring as we used identical information to theirs, only editing the information to be more appropriate for a UK-based audience and with the latest epidemiological information at the time. In both their and our experiment, we found an effect of time spent with the information, in that those who spent longer with the chatbot were more likely to increase their vaccination intention and attitudes. This has potentially important implications for those designing public health information interventions, in that how engaging (and therefore how long participants are willing to attend to the information) is crucial.

TK here there is room to cite studies showing that anti-vaccination attitudes are hard to shift. Thus highlighting the encouraging nature of our result

In contrast to the Altay experiment, we found no difference between our conditions. However, there were crucial differences between our conditions and those of Altay's. The most obvious is that all of our participants saw information of the same length and quality. The fact that our conditions were equally effective then suggests that Altay's chatbot may have been more effective than their control condition not because there is something inherently effective about chatbots, but simply because it delivered more information than the control condition.\\
The second most crucial difference between our experiment and Altay's is that we controlled for trustworthiness of information across both of our conditions. In Altay's chatbot experiment, the chatbot included a question ``why should I trust you?'' in which the participant saw information about who the researchers were and what their motives were. Previous research suggests trust plays a huge role in how effective science communication is (Tom's paper? Other ones? ). The information in Altay's control condition was therefore implicitly less trustworthy than the chatbot information, given the control condition had no source and was anonymous. In contrast, both of our conditions included the ``Why should I trust you?'' question and answer dialogue right at the start of the experiment, before any of the other dialogues were displayed. This dialogue included information about who we (the authors) were, where the information came from, and what our motives were. We also stated that we had no links to pharmaceutical companies or something else, similar to the Altay design. The fact that both of our conditions included this trustworthiness information, and we found no difference between the effectiveness of our conditions, suggests that the trustworthiness of information and/or being transparent about the source of information could be playing an important role.

By design, the only difference between the current study's two conditions was that in one participants had a choice of which information they saw, whereas in the other the information was shown at random. This suggests that having agency or ``choice'' over the information you engage with may not be the most crucial aspect of why chatbots are effective. Furthermore it suggests that addressing the concerns that are of most importance or interest to the participant may not be as crucial as previously thought (although check this with our choice data against our density plots per item. Put those in supplementary info). Previous research suggests participants prefer choice and agency over information when given the choice, but perhaps this preference isn't enough to override the effectiveness of the information when it is given to them without their actively choosing it. Furthermore, we did not find a difference in engagement ratings between our conditions. This suggests that our conditions were not different enough to cause one to be more engaging than the other. Indeed, participants spent a similar amount of time across both conditions. This suggests that making information engaging may be of crucial importance. Seemingly unimportant details of chatbots may account for their being engaging, or at least their ability to hold the attention of the participant. One aspect is the ability to go back and forth between different arguments, or the ability to select numerous counterarguments of interest. Another is the display of the information, which is often more `bitesize' with one sentence delivered at a time, or the ``human'' element of interacting with another `agent' may be inherently more engaging.

It could be argued that in our data all we are seeing is regression to the mean, particularly because we recruited from one end of the vaccination attitude spectrum, and we saw similar effects across both conditions. However, after investigating this possibility, it seems unlikely given that those who were rated as ``against'' vaccination as opposed to ``neutral'' were actually more likely to stay the same in their attitude ratings than the neutral participants, and were less likely than the neutral participants to increase their attitudes (i.e the opposite of what you would expect with regression to the mean, see plot TK reference in supplementary material). Furthermore, not only are our effect sizes similar to Altay's chatbot condition effects, but also much greater than those in previous studies, for example when influenced by norms, participants only showed a 5\% decline in the number of participants who were `undecided' or `against' vaccination (\textbf{Moehring?}), whereas we found a 15\% decline. Previous research also suggests that using Q\&A style information is more effective than presenting pure fact-based information, again reporting similar effects to ours (\textbf{Challenger?}).

One aspect of our study worth noting is how the information was framed and how the participants were addressed throughout the study. Participants were asked if they were against, for, neutral, as is worded in Prolific. We thus used this as our prescreening criteria, so advertised the study as ``your opinions on covid-19 vaccinations.'' Part of our study (results not included for this publication) was to ask participants to give their reasons for and against vaccination, and, to imagine and put forward the opposite side's reasons for and against vaccination (this was conducted after their second round of attitudes, so would not influence the results of this study). We also offered participants an opportunity to provide any other feedback they had. These comments were insightful, and often suggested participants were keen to discuss their views and opinions, and did not have a reliable outlet for this. Anonymity perhaps allowed them to be honest. We also noted many thanked us for not referring to them, or anyone, as ``anti vaccination'' or ``anti-vaxxers.'' We refrained from using this term throughout due to the knowledge that this community is often stereotyped and villainised. We wish to follow-up these comments, respond where necessary, and share them with the rest of the research community to help further destigmatise the anti vaccination community and help create an atmosphere of constructive dialogue and conversational receptiveness about these issues. (refs?)

Overall, we suggest it is important when designing science communication interventions to control for the amount of information and time spent with the information, and consequently to ensure a high level of engagement with the information. Simply providing Q\&A-style dialogues appeared to be as effective as delivering the same information via a chatbot, and more effective than using norms or merely fact-based interventions.

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

Thank you to Dr Andrew Lee from the University of Sheffield who kindly donated his epidemiological expertise and proof-read our updated Covid-19 vaccination FAQs.
Thank you also to Dr Sacha Altay for openly sharing the data and analysis scripts, and being forthcoming with more details and extra information to help run this replication.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-altay2021information}{}%
Altay, S., Hacquin, A.-S., Chevallier, C., \& Mercier, H. (2021). Information delivered by a chatbot has a positive impact on COVID-19 vaccines attitudes and intentions.

\leavevmode\hypertarget{ref-altay_scaling_2020}{}%
Altay, S., Schwartz, M., Hacquin, A.-S., Allard, A., Blancke, S., \& Mercier, H. (2020). Scaling up {Interactive} {Argumentation} by {Providing} {Counterarguments} with a {Chatbot} {[}{Registered} {Report} {Stage} 1 {Protocol}{]}. \url{https://doi.org/10.6084/m9.figshare.13122527.v1}

\leavevmode\hypertarget{ref-baldassarre2014intrinsic}{}%
Baldassarre, G., Stafford, T., Mirolli, M., Redgrave, P., Ryan, R. M., \& Barto, A. (2014). Intrinsic motivations and open-ended development in animals, humans, and robots: An overview. \emph{Frontiers in Psychology}, \emph{5}, 985.

\leavevmode\hypertarget{ref-bobadilla-suarez_intrinsic_2017}{}%
Bobadilla-Suarez, S., Sunstein, C. R., \& Sharot, T. (2017). The intrinsic value of choice: {The} propensity to under-delegate in the face of potential gains and losses. \emph{Journal of Risk and Uncertainty}, \emph{54}(3), 187--202. \url{https://doi.org/10.1007/s11166-017-9259-x}

\leavevmode\hypertarget{ref-burkner2019ordinal}{}%
Bürkner, P.-C., \& Vuorre, M. (2019). Ordinal regression models in psychology: A tutorial. \emph{Advances in Methods and Practices in Psychological Science}, \emph{2}(1), 77--101.

\leavevmode\hypertarget{ref-challenger_covid-19_2021}{}%
Challenger, A. R., Sumner, P., \& Bott, L. (2021). \emph{{COVID}-19 myth-busting: An experimental study} (preprint). In Review. \url{https://doi.org/10.21203/rs.3.rs-790299/v1}

\leavevmode\hypertarget{ref-eiser_trust_2009}{}%
Eiser, J. R., Stafford, T., Henneberry, J., \& Catney, P. (2009). {``{Trust} me, {I}'m a {Scientist} ({Not} a {Developer})''}: {Perceived} {Expertise} and {Motives} as {Predictors} of {Trust} in {Assessment} of {Risk} from {Contaminated} {Land}. \emph{Risk Analysis}, \emph{29}(2), 288--297. \url{https://doi.org/10.1111/j.1539-6924.2008.01131.x}

\leavevmode\hypertarget{ref-mcelreath2018statistical}{}%
McElreath, R. (2018). \emph{Statistical rethinking: A bayesian course with examples in r and stan}. Chapman; Hall/CRC.

\leavevmode\hypertarget{ref-moehring_surfacing_nodate}{}%
Moehring, A., Collis, A., Garimella, K., Rahimian, M. A., Aral, S., \& Eckles, D. (n.d.). Surfacing norms to increase vaccine acceptance, 39.

\leavevmode\hypertarget{ref-thorpe_communicating_2021}{}%
Thorpe, A., Fagerlin, A., Butler, J., Stevens, V., Drews, F. A., Shoemaker, H., \ldots{} Scherer, L. D. (2021). \emph{Communicating about {COVID}-19 vaccine development and safety} (p. 2021.06.25.21259519). \url{https://doi.org/10.1101/2021.06.25.21259519}

\leavevmode\hypertarget{ref-trueblood_role_2020}{}%
Trueblood, J. S., Sussman, A. B., \& O'Leary, D. (2020). \emph{The {Role} of {General} {Risk} {Preferences} in {Messaging} {About} {COVID}-19 {Vaccine} {Take}-{Up}} (\{SSRN\} \{Scholarly\} \{Paper\} No. ID 3649654). Rochester, NY: Social Science Research Network. \url{https://doi.org/10.2139/ssrn.3649654}

\leavevmode\hypertarget{ref-vraga_addressing_2021}{}%
Vraga, E. K., \& Bode, L. (2021). Addressing {COVID}-19 {Misinformation} on {Social} {Media} {Preemptively} and {Responsively}. \emph{Emerging Infectious Diseases}, \emph{27}(2), 396--403. \url{https://doi.org/10.3201/eid2702.203139}

\end{CSLReferences}


\end{document}
